{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"Z2TBLKNZXQUE","cell_type":"code","source":"!pip install --upgrade transformers accelerate bitsandbytes","metadata":{"id":"Z2TBLKNZXQUE","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T21:26:48.862725Z","iopub.execute_input":"2025-11-16T21:26:48.862901Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nCollecting accelerate\n  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.3)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, bitsandbytes, accelerate\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n","output_type":"stream"}],"execution_count":null},{"id":"FtQXj5uRYiVP","cell_type":"markdown","source":"## Import libraries","metadata":{"id":"FtQXj5uRYiVP"}},{"id":"7cXH_E3aZA-L","cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"7cXH_E3aZA-L","trusted":true},"outputs":[],"execution_count":null},{"id":"DrQAAIjXYX1P","cell_type":"code","source":"### Data pre-processing imports ###\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\n### Custom Model Imports ###\nfrom typing import Dict, List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    BitsAndBytesConfig,\n    Trainer,\n    TrainingArguments,\n    TrainerCallback\n)\n\nimport wandb\nimport huggingface_hub\nfrom huggingface_hub import PyTorchModelHubMixin\n\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n)","metadata":{"id":"DrQAAIjXYX1P","trusted":true},"outputs":[],"execution_count":null},{"id":"414db955-eb7f-4b7d-be70-60531f72cfb7","cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n# I have saved my API token with \"wandb_api\" as Label. \n# If you use some other Label make sure to change the same below. \nwandb_api = user_secrets.get_secret(\"wandb_api\") \n\nwandb.login(key=wandb_api)\n\nhuggingface_api = user_secrets.get_secret(\"huggingface_api\") \nhuggingface_hub.login(token=huggingface_api)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0faaaaee-dbeb-404c-a5ab-a523d5de567e","cell_type":"code","source":"# from huggingface_hub import HfApi\n# api = HfApi()\n# api.create_repo(repo_id=\"bheshaj/deberta-v3-base-pairwise-sequence-classifier\", private=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ff74f1ab","cell_type":"code","source":"print(torch.cuda.is_available())","metadata":{"id":"ff74f1ab","trusted":true},"outputs":[],"execution_count":null},{"id":"92fa347a","cell_type":"code","source":"torch.cuda.is_bf16_supported()","metadata":{"id":"92fa347a","trusted":true},"outputs":[],"execution_count":null},{"id":"b63a5062","cell_type":"markdown","source":"## Set config","metadata":{"id":"b63a5062"}},{"id":"a6093ee2-e265-4408-8ff4-d5d850abde70","cell_type":"markdown","source":"Set configurations for model","metadata":{}},{"id":"W2-Rr9nY-kP4","cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Backbone model name\nMODEL_NAME = \"microsoft/deberta-v3-base\"\n\n# Max sequence length for input\nMAX_SEQUENCE_LENGTH = 512\n\n# Use quantization\nUSE_4BIT = True\n\n# PEFT/LoRA Params\nLORA_TARGETS = [\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"]\nLORA_R = 8\nLORA_ALPHA = 16\nLORA_DROPOUT = 0.05","metadata":{"id":"W2-Rr9nY-kP4","trusted":true},"outputs":[],"execution_count":null},{"id":"nRhi5QWbYlz_","cell_type":"markdown","source":"## Import dataset","metadata":{"id":"nRhi5QWbYlz_"}},{"id":"3cwpCmClYpn-","cell_type":"code","source":"raw_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")","metadata":{"id":"3cwpCmClYpn-","trusted":true},"outputs":[],"execution_count":null},{"id":"PeRAsA41ZYwV","cell_type":"code","source":"raw_df.head()","metadata":{"id":"PeRAsA41ZYwV","trusted":true},"outputs":[],"execution_count":null},{"id":"703d326e-43d8-40f7-854c-3f9ca212d846","cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"h9PkNgCQl9MQ","cell_type":"markdown","source":"## EDA","metadata":{"id":"h9PkNgCQl9MQ"}},{"id":"r_-S2ILLl2Cb","cell_type":"code","source":"# Check column data types\nraw_df.dtypes","metadata":{"id":"r_-S2ILLl2Cb","trusted":true},"outputs":[],"execution_count":null},{"id":"9cc02def-a5c9-4aaf-a37f-a0af0878c832","cell_type":"markdown","source":"Count plot of different types models","metadata":{}},{"id":"WcK6GMGtZfNQ","cell_type":"code","source":"plt.figure(figsize=(12,5))\npd.concat([raw_df['model_a'], raw_df['model_b']]).value_counts().plot(kind='bar', stacked=True)\n\nplt.show()","metadata":{"id":"WcK6GMGtZfNQ","trusted":true},"outputs":[],"execution_count":null},{"id":"QEdBb2WhYxHy","cell_type":"markdown","source":"## Data pre-processing","metadata":{"id":"QEdBb2WhYxHy"}},{"id":"2335de3f-4364-4f9a-baf1-1d8fbc16cca1","cell_type":"markdown","source":"Prompt and responses are in json format and need parsing","metadata":{}},{"id":"BA3IByKht4W1","cell_type":"code","source":"import json\n\ndef safe_parse_json(x):\n    if not isinstance(x, str):\n        return x\n    try:\n        val = json.loads(x)\n        # If it's a list, return first non-null element\n        if isinstance(val, list):\n            if val:\n                return [item if item is not None else '' for item in val]\n            else:\n                return ''\n        return val\n    except json.JSONDecodeError:\n        return \"\"\n\nraw_df[\"response_a_processed\"] = raw_df[\"response_a\"].apply(safe_parse_json)\nraw_df[\"response_b_processed\"] = raw_df[\"response_b\"].apply(safe_parse_json)\nraw_df[\"prompt_processed\"] = raw_df[\"prompt\"].apply(safe_parse_json)","metadata":{"id":"BA3IByKht4W1","trusted":true},"outputs":[],"execution_count":null},{"id":"343b80f7-8eac-451b-b8c4-a9d9f7141080","cell_type":"code","source":"test_df[\"response_a_processed\"] = test_df[\"response_a\"].apply(safe_parse_json)\ntest_df[\"response_b_processed\"] = test_df[\"response_b\"].apply(safe_parse_json)\ntest_df[\"prompt_processed\"] = test_df[\"prompt\"].apply(safe_parse_json)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ea87de3a-5ea7-4bd7-938d-c3b04820af78","cell_type":"markdown","source":"Format query and response in a question-answer format like in a chat\n\nEx-\n\nQuery:\nWhat is the difference between marriage license and marriage certificate?\n\nResponse:\nA marriage license is a legal document that allows a couple to get married. It is issued by a government agency, such as a county clerk's office or a state government, and is valid for a certain period of time, usually one year. After the marriage has taken place, the couple must obtain a marriage certificate, which is a document that records the marriage and is used to prove that the marriage took place. The marriage certificate is usually issued by the same government agency that issued the marriage license, and it is typically used for legal purposes, such as to change a name on a driver's license or to prove that a couple is married when applying for government benefits.","metadata":{}},{"id":"JBBbizlp6Kb5","cell_type":"code","source":"def format_conversation(query_list, response_list):\n    parts = []\n    for i, (q, r) in enumerate(zip(query_list, response_list)):\n        parts.append((f\"Query:\\n{q}\\n\\nResponse:\\n{r}\"))\n    return '\\n\\n'.join(parts)\n\nraw_df['text_a'] = raw_df.apply(lambda x: format_conversation(x['prompt_processed'], x['response_a_processed']), axis=1)\nraw_df['text_b'] = raw_df.apply(lambda x: format_conversation(x['prompt_processed'], x['response_b_processed']), axis=1)","metadata":{"id":"JBBbizlp6Kb5","trusted":true},"outputs":[],"execution_count":null},{"id":"3c36653b-6ec2-4f1e-8ab2-4f5901b7f931","cell_type":"code","source":"test_df['text_a'] = test_df.apply(lambda x: format_conversation(x['prompt_processed'], x['response_a_processed']), axis=1)\ntest_df['text_b'] = test_df.apply(lambda x: format_conversation(x['prompt_processed'], x['response_b_processed']), axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4175539c-3c96-4f38-a7f8-705fe3826e4f","cell_type":"markdown","source":"Summary statistics for the number of words in the conversation texts. This helps to determine the maximum input sequence length to the model and hence helps decide the model to choose.\n\nThe conversations mostly have < 550 words in each conversation. Assuming $ \\text{Tokens per conversation} = 1.5 \\times \\text{Words per conversation} $, we would ideally need a model which can handle ~850 tokens. However, to keep the model easy and simple to train we will use a model with smaller max sequence limit.","metadata":{}},{"id":"6f80bb8f","cell_type":"code","source":"word_split = raw_df[\"text_a\"].apply(lambda x: x.split(' '))\nword_split.apply(lambda x: len(x)).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.90])","metadata":{"id":"6f80bb8f","trusted":true},"outputs":[],"execution_count":null},{"id":"To4kNLrf51xr","cell_type":"code","source":"word_split = raw_df[\"text_b\"].apply(lambda x: x.split(' '))\nword_split.apply(lambda x: len(x)).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.90])","metadata":{"id":"To4kNLrf51xr","trusted":true},"outputs":[],"execution_count":null},{"id":"73c2697e","cell_type":"markdown","source":"### Create target label\n\nCreate a single target column which can help determine the true class","metadata":{"id":"73c2697e"}},{"id":"mo-_Gt67-exC","cell_type":"code","source":"def create_target_col(encoding):\n    \"\"\"\n    Create column for target labels\n    \"\"\"\n\n    if encoding == [0, 0, 1]:\n        return 'tie'\n    elif encoding == [0, 1, 0]:\n        return 'model_b'\n    elif encoding == [1, 0, 0]:\n        return 'model_a'\n\n    return np.nan\n\nraw_df['target'] = raw_df[['winner_model_a', 'winner_model_b', 'winner_tie']].apply(lambda x: create_target_col(list(x)), axis=1)\n\nraw_df['label'] = raw_df['target'].map({'model_a': 0, 'model_b': 1, 'tie': 2})","metadata":{"id":"mo-_Gt67-exC","trusted":true},"outputs":[],"execution_count":null},{"id":"85327fba-57e5-4c8e-936d-e2855bfe15c9","cell_type":"markdown","source":"### Train-test split","metadata":{}},{"id":"5d0f8b1e-e463-4eaf-89b2-66b8f74ab7c4","cell_type":"code","source":"\ntrain_df, eval_df = train_test_split(raw_df, test_size=0.2, random_state=42, stratify=raw_df[\"label\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"acb2b59b-287f-48b1-a7e0-2fd8882e496c","cell_type":"markdown","source":"## Model Architecture","metadata":{"id":"KVhepZmjzjR6"}},{"id":"V9ke4STbziyW","cell_type":"code","source":"def mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n    summed = (last_hidden_state * mask).sum(dim=1)\n    count = mask.sum(dim=1).clamp(min=1e-9)\n    return summed / count\n\nclass PairwiseBiEncoder(\n    nn.Module, \n    PyTorchModelHubMixin # Required to push the model to huggingface hub\n    ):\n    \"\"\"\n    Two independent encodes + comparison head -> 3-way logits.\n    Expects tokenized dicts for A and B: {input_ids, attention_mask}.\n    \"\"\"\n    def __init__(self, encoder: nn.Module, hidden_size: int, num_labels: int = 3, dropout: float = 0.2):\n        super().__init__()\n        self.num_labels = num_labels\n        self.encoder = encoder\n        self.dropout = nn.Dropout(dropout)\n        self.config = getattr(encoder, \"config\", None) # Needed for peft/LoRA config\n        self.classifier = nn.Sequential(\n            nn.Linear(4 * hidden_size, 2*hidden_size),\n            nn.GELU(),\n            nn.Linear(2 * hidden_size, hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size // 2, self.num_labels)\n        )\n\n    def freeze_backbone(self):\n        for name, param in self.encoder.named_parameters():\n            # skip non-floating params (e.g., int8/quantized wrappers)\n            dt = getattr(param, \"dtype\", None)\n            if dt is None or not dt.is_floating_point:\n                # leave as-is (cannot require grad)\n                continue\n            param.requires_grad = False\n\n    def unfreeze_backbone(self):\n        skipped = []\n        for name, param in self.encoder.named_parameters():\n            dt = getattr(param, \"dtype\", None)\n            if dt is None or not dt.is_floating_point:\n                skipped.append(name)\n                continue\n            param.requires_grad = True\n        if skipped:\n            # small debug print — in heavy logging environments prefer logger.warning\n            print(f\"Warning: skipped unfreezing {len(skipped)} non-float params (examples): {skipped[:6]}\")\n\n\n    def is_backbone_frozen(self) -> bool:\n        return not any(p.requires_grad for p in self.encoder.parameters())\n\n    def _encode(self, input_ids, attention_mask):\n        # ensure return_dict=True for HF models; some wrappers still return tuple\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n\n        # handle tuple outputs (PEFT/LoRA/backbone wrappers sometimes return tuple)\n        if isinstance(out, tuple):\n            last_hidden_state = out[0]\n            pooler_output = out[1] if len(out) > 1 else None\n        else:\n            last_hidden_state = getattr(out, \"last_hidden_state\", None)\n            pooler_output = getattr(out, \"pooler_output\", None)\n\n        if pooler_output is not None:\n            return pooler_output\n        if last_hidden_state is not None:\n            return mean_pool(last_hidden_state, attention_mask)\n\n        raise ValueError(\"Encoder output missing last_hidden_state and pooler_output.\")\n\n    def forward(\n        self,\n        a_input_ids: torch.Tensor,\n        a_attention_mask: torch.Tensor,\n        b_input_ids: torch.Tensor,\n        b_attention_mask: torch.Tensor,\n        labels: Optional[torch.Tensor] = None,\n        **kwargs,  # catch everything else, needed with peft\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Standard forward pass compatible with HF Trainer.\n        \"\"\"\n        hA = self._encode(a_input_ids, a_attention_mask)  # [B, H]\n        hB = self._encode(b_input_ids, b_attention_mask)  # [B, H]\n\n        comb = torch.cat([hA, hB, torch.abs(hA - hB), hA * hB], dim=-1)\n        logits = self.classifier(self.dropout(comb))\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        return {\"loss\": loss, \"logits\": logits}\n\nclass FreezeUnfreezeCallback(TrainerCallback):\n    \"\"\"\n    Freezes backbone for initial epochs, then unfreezes later.\n    \"\"\"\n\n    def __init__(self, unfreeze_at_epoch: int = 1):\n        self.unfreeze_at_epoch = unfreeze_at_epoch\n        self.has_unfrozen = False\n\n    def on_epoch_begin(self, args, state, control, model=None, **kwargs):\n        if state.epoch < self.unfreeze_at_epoch:\n            if not model.is_backbone_frozen():\n                print(f\"Epoch {int(state.epoch)}: Freezing backbone.\")\n                model.freeze_backbone()\n        elif not self.has_unfrozen:\n            print(f\"Epoch {int(state.epoch)}: Unfreezing backbone.\")\n            model.unfreeze_backbone()\n            self.has_unfrozen = True","metadata":{"id":"V9ke4STbziyW","trusted":true},"outputs":[],"execution_count":null},{"id":"da5f1438-4822-4e2b-8eab-103ec8f62b26","cell_type":"markdown","source":"### Quantization and LoRA config","metadata":{}},{"id":"0695aaab-ff4e-48e2-9bb5-33b58761cc5e","cell_type":"code","source":"quant_config = BitsAndBytesConfig(\n    load_in_4bit=USE_4BIT,\n    load_in_8bit=not USE_4BIT,\n    bnb_4bit_quant_type=\"nf4\" if USE_4BIT else None,\n    bnb_4bit_use_double_quant=True if USE_4BIT else None,\n    bnb_4bit_compute_dtype=torch.bfloat16 if USE_4BIT and torch.cuda.is_available() else None,\n)\n\n# Apply LoRA\nlora_cfg = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    target_modules=LORA_TARGETS,\n    bias=\"none\",\n    task_type=\"SEQ_CLS\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"31cf8699-215a-4f2f-9b48-fd8d1b76432d","cell_type":"markdown","source":"### Initialize backbone model and tokenizer","metadata":{}},{"id":"c610747b-d4b6-46b5-86a1-76a0ecfa61c4","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nbackbone = AutoModel.from_pretrained(MODEL_NAME, quantization_config=quant_config, device_map=\"auto\")\n\nhidden_size = backbone.config.hidden_size\n\nassert MAX_SEQUENCE_LENGTH <= backbone.config.max_position_embeddings, f\"Config 'max_sequence_length' must be <= the max sequence length allowed by the model i.e. {backbone.config.max_position_embeddings}\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"56cff569-9f9c-43e7-bcd8-948b9cbded69","cell_type":"markdown","source":"### Tokenize and format data as per requirement in huggingface trainer","metadata":{}},{"id":"NPhh67GpWAZg","cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List, Any\nimport torch\n\ndef tokenize_pairwise(batch):\n    a_encodings = tokenizer(\n        batch[\"text_a\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=MAX_SEQUENCE_LENGTH,\n    )\n    b_encodings = tokenizer(\n        batch[\"text_b\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=MAX_SEQUENCE_LENGTH,\n    )\n\n    out_dict = {\n        \"a_input_ids\": a_encodings[\"input_ids\"],\n        \"a_attention_mask\": a_encodings[\"attention_mask\"],\n        \"b_input_ids\": b_encodings[\"input_ids\"],\n        \"b_attention_mask\": b_encodings[\"attention_mask\"]\n    }\n\n    if \"label\" in batch:\n        out_dict[\"labels\"] = batch[\"label\"]\n    \n    return out_dict\n","metadata":{"id":"NPhh67GpWAZg","trusted":true},"outputs":[],"execution_count":null},{"id":"f17ff1e1-445b-461f-bc8e-c9d096f1bc4c","cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_pandas(train_df)\neval_dataset = Dataset.from_pandas(eval_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"14fef515-ec17-44ec-94cb-7207e602c178","cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import List, Any, Dict\nimport torch\n\n@dataclass\nclass PairwiseDataCollator:\n    tokenizer: Any\n    padding: str = \"longest\"   # \"longest\" or \"max_length\"\n    max_length: int = 128\n\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        # defensive: filter out None\n        features = [f for f in features if f is not None and isinstance(f, dict)]\n        if len(features) == 0:\n            raise ValueError(\"Empty batch after filtering None features in collator.\")\n\n        # ensure required keys exist\n        for i, f in enumerate(features):\n            for k in (\"a_input_ids\", \"a_attention_mask\", \"b_input_ids\", \"b_attention_mask\"):\n                if k not in f:\n                    raise ValueError(f\"Missing key {k} in feature at batch pos {i}: keys={list(f.keys())}\")\n\n        a_feats = [{\"input_ids\": f[\"a_input_ids\"], \"attention_mask\": f[\"a_attention_mask\"]} for f in features]\n        b_feats = [{\"input_ids\": f[\"b_input_ids\"], \"attention_mask\": f[\"b_attention_mask\"]} for f in features]\n\n        # NOTE: truncation should have been done during tokenization (when creating the dataset).\n        # pad() does not take `truncation` argument on many tokenizers, so we remove it here.\n        pad_kwargs = {\"padding\": self.padding, \"return_tensors\": \"pt\"}\n        if self.padding == \"max_length\":\n            pad_kwargs[\"max_length\"] = self.max_length\n\n        a_batch = self.tokenizer.pad(a_feats, **pad_kwargs)\n        b_batch = self.tokenizer.pad(b_feats, **pad_kwargs)\n\n        out = {\n            \"a_input_ids\": a_batch[\"input_ids\"],\n            \"a_attention_mask\": a_batch[\"attention_mask\"],\n            \"b_input_ids\": b_batch[\"input_ids\"],\n            \"b_attention_mask\": b_batch[\"attention_mask\"],\n        }\n\n        # optional labels\n        if \"labels\" in features[0]:\n            labels = [int(f[\"labels\"]) for f in features]\n            out[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n\n        # carry ids if present (not tensorized)\n        if \"id\" in features[0]:\n            out[\"ids\"] = [f.get(\"id\") for f in features]\n\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"RJcJEOJNGe0G","cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_pandas(train_df)\neval_dataset = Dataset.from_pandas(eval_df)\n\ntrain_dataset = train_dataset.map(tokenize_pairwise, batched=True, remove_columns=list(train_df.columns))\neval_dataset = eval_dataset.map(tokenize_pairwise, batched=True, remove_columns=list(eval_df.columns))\n","metadata":{"id":"RJcJEOJNGe0G","trusted":true},"outputs":[],"execution_count":null},{"id":"EMuDIDS4GjvU","cell_type":"code","source":"train_dataset.set_format(\n    type=\"torch\",\n    columns=[\"a_input_ids\",\"a_attention_mask\",\"b_input_ids\",\"b_attention_mask\",\"labels\"]\n)\n\neval_dataset.set_format(\n    type=\"torch\",\n    columns=[\"a_input_ids\",\"a_attention_mask\",\"b_input_ids\",\"b_attention_mask\",\"labels\"]\n)","metadata":{"id":"EMuDIDS4GjvU","trusted":true},"outputs":[],"execution_count":null},{"id":"062aa4d6-53fa-4fb1-ace5-b1daae0fd40d","cell_type":"code","source":"test_dataset = Dataset.from_pandas(test_df)\n\ntest_dataset = test_dataset.map(tokenize_pairwise, batched=True, remove_columns=list(test_df.columns))\n\ntest_dataset.set_format(\n    type=\"torch\",\n    columns=[\"a_input_ids\",\"a_attention_mask\",\"b_input_ids\",\"b_attention_mask\"]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"pVGa7crZtS0x","cell_type":"code","source":"# Prepare backbone for k-bit training\nbackbone = prepare_model_for_kbit_training(backbone)\n\n# Create custom model\ncustom_model = PairwiseBiEncoder(encoder=backbone, hidden_size=backbone.config.hidden_size, num_labels=3)\n\n# Ensure the custom model has a .config (PEFT expects it)\ncustom_model.config = backbone.config\n\n# Prepare model for quantization and LoRA\ncustom_model = get_peft_model(custom_model, lora_cfg)","metadata":{"id":"pVGa7crZtS0x","trusted":true},"outputs":[],"execution_count":null},{"id":"lrDtVobWXb_N","cell_type":"code","source":"data_collator = PairwiseDataCollator(tokenizer=tokenizer)\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/deberta-v3-base-pairwise-sequence-classifier\",\n    num_train_epochs=6,\n    per_device_train_batch_size=16,\n    eval_strategy=\"epoch\",\n    learning_rate=5e-5,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.05,\n    report_to='wandb',\n    push_to_hub=True,\n    hub_model_id='bheshaj/deberta-v3-base-pairwise-sequence-classifier',\n    hub_private_repo=False,\n    save_strategy=\"epoch\",\n    save_total_limit=1,           # keep last n checkpoints (optional)\n    load_best_model_at_end=True,    # optional\n    save_safetensors=True,           # recommended\n)\n\ntrainer = Trainer(\n    model=custom_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    callbacks=[FreezeUnfreezeCallback(unfreeze_at_epoch=3)],  # ⬅️ add callback here\n)\n\ntrainer.train()","metadata":{"id":"lrDtVobWXb_N","trusted":true},"outputs":[],"execution_count":null},{"id":"8dd2e2da-8c17-4ea2-9df0-2be18491195d","cell_type":"code","source":"trainer.push_to_hub()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"097ff281-0b58-4a50-b409-dd27b000d5a4","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}