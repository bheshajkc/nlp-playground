{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z2TBLKNZXQUE",
      "metadata": {
        "id": "Z2TBLKNZXQUE"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FtQXj5uRYiVP",
      "metadata": {
        "id": "FtQXj5uRYiVP"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cXH_E3aZA-L",
      "metadata": {
        "id": "7cXH_E3aZA-L"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DrQAAIjXYX1P",
      "metadata": {
        "id": "DrQAAIjXYX1P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff74f1ab",
      "metadata": {
        "id": "ff74f1ab"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92fa347a",
      "metadata": {
        "id": "92fa347a"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_bf16_supported()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b63a5062",
      "metadata": {
        "id": "b63a5062"
      },
      "source": [
        "## Set config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88762c32",
      "metadata": {
        "id": "88762c32"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 512"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Aotmfl5S7mFH"
      },
      "id": "Aotmfl5S7mFH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
        "USE_4BIT = True\n",
        "\n",
        "LORA_TARGETS = [\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"]\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05"
      ],
      "metadata": {
        "id": "W2-Rr9nY-kP4"
      },
      "id": "W2-Rr9nY-kP4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "nRhi5QWbYlz_",
      "metadata": {
        "id": "nRhi5QWbYlz_"
      },
      "source": [
        "## Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea15b34",
      "metadata": {
        "id": "3ea15b34"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cwpCmClYpn-",
      "metadata": {
        "id": "3cwpCmClYpn-"
      },
      "outputs": [],
      "source": [
        "raw_df = pd.read_csv(\"/content/drive/MyDrive/Data Science Projects/llm-finetuning/nlp-playground/data/raw/llm-classification-finetuning/train.csv\")\n",
        "# raw_df = pd.read_csv(\"./data/raw/llm-classification-finetuning/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PeRAsA41ZYwV",
      "metadata": {
        "id": "PeRAsA41ZYwV"
      },
      "outputs": [],
      "source": [
        "raw_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h9PkNgCQl9MQ",
      "metadata": {
        "id": "h9PkNgCQl9MQ"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r_-S2ILLl2Cb",
      "metadata": {
        "id": "r_-S2ILLl2Cb"
      },
      "outputs": [],
      "source": [
        "raw_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3Z3E2wxmLBR",
      "metadata": {
        "id": "f3Z3E2wxmLBR"
      },
      "outputs": [],
      "source": [
        "print(type(raw_df['prompt'].iloc[0]))\n",
        "print(type(raw_df['response_a'].iloc[0]))\n",
        "print(type(raw_df['response_b'].iloc[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WcK6GMGtZfNQ",
      "metadata": {
        "id": "WcK6GMGtZfNQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "pd.concat([raw_df['model_a'], raw_df['model_b']]).value_counts().plot(kind='bar', stacked=True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QEdBb2WhYxHy",
      "metadata": {
        "id": "QEdBb2WhYxHy"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BA3IByKht4W1",
      "metadata": {
        "id": "BA3IByKht4W1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def safe_parse_json(x):\n",
        "    if not isinstance(x, str):\n",
        "        return x\n",
        "    try:\n",
        "        val = json.loads(x)\n",
        "        # If it's a list, return first non-null element\n",
        "        if isinstance(val, list):\n",
        "            if val:\n",
        "                return [item if item is not None else '' for item in val]\n",
        "            else:\n",
        "                return ''\n",
        "        return val\n",
        "    except json.JSONDecodeError:\n",
        "        return \"\"\n",
        "\n",
        "raw_df[\"response_a_processed\"] = raw_df[\"response_a\"].apply(safe_parse_json)\n",
        "raw_df[\"response_b_processed\"] = raw_df[\"response_b\"].apply(safe_parse_json)\n",
        "raw_df[\"prompt_processed\"] = raw_df[\"prompt\"].apply(safe_parse_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JBBbizlp6Kb5",
      "metadata": {
        "id": "JBBbizlp6Kb5"
      },
      "outputs": [],
      "source": [
        "def format_conversation(query_list, response_list):\n",
        "    parts = []\n",
        "    for i, (q, r) in enumerate(zip(query_list, response_list)):\n",
        "        parts.append((f\"Query:\\n{q}\\n\\nResponse:\\n{r}\"))\n",
        "    return '\\n\\n'.join(parts)\n",
        "\n",
        "raw_df['text_a'] = raw_df.apply(lambda x: format_conversation(x['prompt_processed'], x['response_a_processed']), axis=1)\n",
        "raw_df['text_b'] = raw_df.apply(lambda x: format_conversation(x['prompt_processed'], x['response_b_processed']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f80bb8f",
      "metadata": {
        "id": "6f80bb8f"
      },
      "outputs": [],
      "source": [
        "word_split = raw_df[\"text_a\"].apply(lambda x: x.split(' '))\n",
        "word_split.apply(lambda x: len(x)).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.90])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_split = raw_df[\"text_b\"].apply(lambda x: x.split(' '))\n",
        "word_split.apply(lambda x: len(x)).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.90])"
      ],
      "metadata": {
        "id": "To4kNLrf51xr"
      },
      "id": "To4kNLrf51xr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "73c2697e",
      "metadata": {
        "id": "73c2697e"
      },
      "source": [
        "The conversations mostly have < 1000 words in each conversation. Assuming $ \\text{Tokens per conversation} = 1.5 \\times \\text{Words per conversation} $, we would ideally need a model which can handle ~1500 tokens. However, to keep the model easy and simpleto train we will use a model with smaller max sequence limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mo-_Gt67-exC",
      "metadata": {
        "id": "mo-_Gt67-exC"
      },
      "outputs": [],
      "source": [
        "def create_target_col(encoding):\n",
        "    \"\"\"\n",
        "    Create column for target labels\n",
        "    \"\"\"\n",
        "\n",
        "    if encoding == [0, 0, 1]:\n",
        "        return 'tie'\n",
        "    elif encoding == [0, 1, 0]:\n",
        "        return 'model_b'\n",
        "    elif encoding == [1, 0, 0]:\n",
        "        return 'model_a'\n",
        "\n",
        "    return np.nan\n",
        "\n",
        "raw_df['target'] = raw_df[['winner_model_a', 'winner_model_b', 'winner_tie']].apply(lambda x: create_target_col(list(x)), axis=1)\n",
        "\n",
        "raw_df['label'] = raw_df['target'].map({'model_a': 0, 'model_b': 1, 'tie': 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up modelling architecture"
      ],
      "metadata": {
        "id": "YzjnNhim6GFd"
      },
      "id": "YzjnNhim6GFd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iU5dI8RGAo9V",
      "metadata": {
        "id": "iU5dI8RGAo9V"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "\n",
        "# Get model for embeddings\n",
        "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "load_in_4bit=USE_4BIT,\n",
        "load_in_8bit=not USE_4BIT,\n",
        "bnb_4bit_quant_type=\"nf4\" if USE_4BIT else None,\n",
        "bnb_4bit_use_double_quant=True if USE_4BIT else None,\n",
        "bnb_4bit_compute_dtype=torch.bfloat16 if USE_4BIT and torch.cuda.is_available() else None,\n",
        ")\n",
        "\n",
        "backbone = AutoModel.from_pretrained(MODEL_NAME, quantization_config=quant_config, device_map=\"auto\")\n",
        "\n",
        "# Prepare for k-bit training (fixes layer norms, casts, etc.)\n",
        "# backbone = prepare_model_for_kbit_training(backbone)\n",
        "\n",
        "# Apply LoRA\n",
        "lora_cfg = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=LORA_TARGETS,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\", # generic; works for encoder models\n",
        ")\n",
        "\n",
        "# backbone = get_peft_model(backbone, lora_cfg)\n",
        "\n",
        "hidden_size = backbone.config.hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert MAX_SEQUENCE_LENGTH <= backbone.config.max_position_embeddings, f\"Config 'max_sequence_length' must be <= the max sequence length allowed by the model i.e. {backbone.config.max_position_embeddings}\""
      ],
      "metadata": {
        "id": "MQ40LbePugPj"
      },
      "id": "MQ40LbePugPj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Model Architecture"
      ],
      "metadata": {
        "id": "KVhepZmjzjR6"
      },
      "id": "KVhepZmjzjR6"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
        "    summed = (last_hidden_state * mask).sum(dim=1)\n",
        "    count = mask.sum(dim=1).clamp(min=1e-9)\n",
        "    return summed / count\n",
        "\n",
        "class PairwiseBiEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Two independent encodes + comparison head -> 3-way logits.\n",
        "    Expects tokenized dicts for A and B: {input_ids, attention_mask}.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder: nn.Module, hidden_size: int, num_labels: int = 3, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.encoder = encoder\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.config = encoder.config # Needed for peft/LoRA config\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4 * hidden_size, 2*hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(2 * hidden_size, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size // 2, self.num_labels)\n",
        "        )\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_backbone(self):\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def is_backbone_frozen(self) -> bool:\n",
        "        return not any(p.requires_grad for p in self.encoder.parameters())\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _encode(self, input_ids, attention_mask):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "\n",
        "        # Prefer mean pooling for stability across backbones\n",
        "        if hasattr(out, \"pooler_output\") and out.pooler_output is not None:\n",
        "            # Some models expose .pooler_output\n",
        "            pooled = out.pooler_output\n",
        "        else:\n",
        "            pooled = mean_pool(out.last_hidden_state, attention_mask)\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        a_input_ids: torch.Tensor,\n",
        "        a_attention_mask: torch.Tensor,\n",
        "        b_input_ids: torch.Tensor,\n",
        "        b_attention_mask: torch.Tensor,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        **kwargs,  # catch everything else, needed with peft\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Standard forward pass compatible with HF Trainer.\n",
        "        \"\"\"\n",
        "        hA = self._encode(a_input_ids, a_attention_mask)  # [B, H]\n",
        "        hB = self._encode(b_input_ids, b_attention_mask)  # [B, H]\n",
        "\n",
        "        comb = torch.cat([hA, hB, torch.abs(hA - hB), hA * hB], dim=-1)\n",
        "        logits = self.classifier(self.dropout(comb))\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}"
      ],
      "metadata": {
        "id": "V9ke4STbziyW"
      },
      "id": "V9ke4STbziyW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class FreezeUnfreezeCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    Freezes backbone for initial epochs, then unfreezes later.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, unfreeze_at_epoch: int = 1):\n",
        "        self.unfreeze_at_epoch = unfreeze_at_epoch\n",
        "        self.has_unfrozen = False\n",
        "\n",
        "    def on_epoch_begin(self, args, state, control, model=None, **kwargs):\n",
        "        if state.epoch < self.unfreeze_at_epoch:\n",
        "            if not model.is_backbone_frozen():\n",
        "                print(f\"Epoch {int(state.epoch)}: Freezing backbone.\")\n",
        "                model.freeze_backbone()\n",
        "        elif not self.has_unfrozen:\n",
        "            print(f\"Epoch {int(state.epoch)}: Unfreezing backbone.\")\n",
        "            model.unfreeze_backbone()\n",
        "            self.has_unfrozen = True"
      ],
      "metadata": {
        "id": "4ItqUnf-u0OJ"
      },
      "id": "4ItqUnf-u0OJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NPhh67GpWAZg",
      "metadata": {
        "id": "NPhh67GpWAZg"
      },
      "outputs": [],
      "source": [
        "def tokenize_pairwise(batch):\n",
        "    a_encodings = tokenizer(\n",
        "        batch[\"text_a\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "    )\n",
        "    b_encodings = tokenizer(\n",
        "        batch[\"text_b\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"a_input_ids\": a_encodings[\"input_ids\"],\n",
        "        \"a_attention_mask\": a_encodings[\"attention_mask\"],\n",
        "        \"b_input_ids\": b_encodings[\"input_ids\"],\n",
        "        \"b_attention_mask\": b_encodings[\"attention_mask\"],\n",
        "        \"labels\": batch[\"label\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qtkHoMLnWA34",
      "metadata": {
        "id": "qtkHoMLnWA34"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, eval_df = train_test_split(raw_df, test_size=0.2, random_state=42, stratify=raw_df[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_pairwise, batched=True, remove_columns=list(train_df.columns))\n",
        "eval_dataset = eval_dataset.map(tokenize_pairwise, batched=True, remove_columns=list(eval_df.columns))\n"
      ],
      "metadata": {
        "id": "RJcJEOJNGe0G"
      },
      "id": "RJcJEOJNGe0G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.set_format(type=\"torch\")\n",
        "\n",
        "eval_dataset.set_format(type=\"torch\")\n"
      ],
      "metadata": {
        "id": "EMuDIDS4GjvU"
      },
      "id": "EMuDIDS4GjvU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class PairwiseDataCollator:\n",
        "    tokenizer: Any\n",
        "    padding: bool = True\n",
        "    max_length: int = 128\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        # Extract A and B separately\n",
        "        a_features = [\n",
        "            {\"input_ids\": f[\"a_input_ids\"], \"attention_mask\": f[\"a_attention_mask\"]}\n",
        "            for f in features\n",
        "        ]\n",
        "        b_features = [\n",
        "            {\"input_ids\": f[\"b_input_ids\"], \"attention_mask\": f[\"b_attention_mask\"]}\n",
        "            for f in features\n",
        "        ]\n",
        "\n",
        "        # Pad each side independently\n",
        "        a_batch = self.tokenizer.pad(\n",
        "            a_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        b_batch = self.tokenizer.pad(\n",
        "            b_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Collect labels (ensure tensor)\n",
        "        labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
        "\n",
        "        # Return combined dict compatible with your model’s forward()\n",
        "        return {\n",
        "            \"a_input_ids\": a_batch[\"input_ids\"],\n",
        "            \"a_attention_mask\": a_batch[\"attention_mask\"],\n",
        "            \"b_input_ids\": b_batch[\"input_ids\"],\n",
        "            \"b_attention_mask\": b_batch[\"attention_mask\"],\n",
        "            \"labels\": labels,\n",
        "        }"
      ],
      "metadata": {
        "id": "-tM1d9r2kfyH"
      },
      "id": "-tM1d9r2kfyH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModel, AutoTokenizer\n",
        "\n",
        "# encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
        "custom_model = PairwiseBiEncoder(encoder=backbone, hidden_size=backbone.config.hidden_size, num_labels=3)\n",
        "custom_model = prepare_model_for_kbit_training(custom_model)\n",
        "custom_model = get_peft_model(custom_model, lora_cfg)\n",
        "\n",
        "data_collator = PairwiseDataCollator(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "pVGa7crZtS0x"
      },
      "id": "pVGa7crZtS0x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=8,\n",
        "    per_device_train_batch_size=16,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=custom_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[FreezeUnfreezeCallback(unfreeze_at_epoch=3)],  # ⬅️ add callback here\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "lrDtVobWXb_N"
      },
      "id": "lrDtVobWXb_N",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cuda_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}