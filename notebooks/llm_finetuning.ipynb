{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z2TBLKNZXQUE",
      "metadata": {
        "id": "Z2TBLKNZXQUE"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FtQXj5uRYiVP",
      "metadata": {
        "id": "FtQXj5uRYiVP"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cXH_E3aZA-L",
      "metadata": {
        "id": "7cXH_E3aZA-L"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DrQAAIjXYX1P",
      "metadata": {
        "id": "DrQAAIjXYX1P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff74f1ab",
      "metadata": {
        "id": "ff74f1ab"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92fa347a",
      "metadata": {
        "id": "92fa347a"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_bf16_supported()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b63a5062",
      "metadata": {
        "id": "b63a5062"
      },
      "source": [
        "## Set config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88762c32",
      "metadata": {
        "id": "88762c32"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 512"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Aotmfl5S7mFH"
      },
      "id": "Aotmfl5S7mFH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
        "USE_4BIT = True\n",
        "\n",
        "LORA_TARGETS = [\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"]\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05"
      ],
      "metadata": {
        "id": "W2-Rr9nY-kP4"
      },
      "id": "W2-Rr9nY-kP4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "nRhi5QWbYlz_",
      "metadata": {
        "id": "nRhi5QWbYlz_"
      },
      "source": [
        "## Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea15b34",
      "metadata": {
        "id": "3ea15b34"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cwpCmClYpn-",
      "metadata": {
        "id": "3cwpCmClYpn-"
      },
      "outputs": [],
      "source": [
        "raw_df = pd.read_csv(\"/content/drive/MyDrive/Data Science Projects/llm-finetuning/nlp-playground/data/raw/llm-classification-finetuning/train.csv\")\n",
        "# raw_df = pd.read_csv(\"./data/raw/llm-classification-finetuning/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PeRAsA41ZYwV",
      "metadata": {
        "id": "PeRAsA41ZYwV"
      },
      "outputs": [],
      "source": [
        "raw_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h9PkNgCQl9MQ",
      "metadata": {
        "id": "h9PkNgCQl9MQ"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r_-S2ILLl2Cb",
      "metadata": {
        "id": "r_-S2ILLl2Cb"
      },
      "outputs": [],
      "source": [
        "raw_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3Z3E2wxmLBR",
      "metadata": {
        "id": "f3Z3E2wxmLBR"
      },
      "outputs": [],
      "source": [
        "print(type(raw_df['prompt'].iloc[0]))\n",
        "print(type(raw_df['response_a'].iloc[0]))\n",
        "print(type(raw_df['response_b'].iloc[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WcK6GMGtZfNQ",
      "metadata": {
        "id": "WcK6GMGtZfNQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "pd.concat([raw_df['model_a'], raw_df['model_b']]).value_counts().plot(kind='bar', stacked=True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fc4AWLUFnwcH",
      "metadata": {
        "id": "Fc4AWLUFnwcH"
      },
      "outputs": [],
      "source": [
        "raw_df.loc[raw_df['response_a'].str.len() < 10, 'response_a'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QEdBb2WhYxHy",
      "metadata": {
        "id": "QEdBb2WhYxHy"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BA3IByKht4W1",
      "metadata": {
        "id": "BA3IByKht4W1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def safe_parse_json(x):\n",
        "    if not isinstance(x, str):\n",
        "        return x\n",
        "    try:\n",
        "        val = json.loads(x)\n",
        "        # If it's a list, return first non-null element\n",
        "        if isinstance(val, list):\n",
        "            if val:\n",
        "                return [item if item is not None else '' for item in val]\n",
        "            else:\n",
        "                return ''\n",
        "        return val\n",
        "    except json.JSONDecodeError:\n",
        "        return \"\"\n",
        "\n",
        "raw_df[\"response_a_processed\"] = raw_df[\"response_a\"].apply(safe_parse_json)\n",
        "raw_df[\"response_b_processed\"] = raw_df[\"response_b\"].apply(safe_parse_json)\n",
        "raw_df[\"prompt_processed\"] = raw_df[\"prompt\"].apply(safe_parse_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IPThHWmi_5ht",
      "metadata": {
        "id": "IPThHWmi_5ht"
      },
      "outputs": [],
      "source": [
        "# Check the number of queries and responses in each row\n",
        "len_resp = raw_df[\"response_a_processed\"].apply(lambda x: len(x))\n",
        "\n",
        "len_resp.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JBBbizlp6Kb5",
      "metadata": {
        "id": "JBBbizlp6Kb5"
      },
      "outputs": [],
      "source": [
        "def format_conversation(query_list, response_list):\n",
        "    parts = []\n",
        "    for i, (q, r) in enumerate(zip(query_list, response_list)):\n",
        "        parts.append((f\"Query:\\n{q}\\n\\nResponse:\\n{r}\"))\n",
        "    return '\\n\\n'.join(parts)\n",
        "\n",
        "raw_df['conversation_a'] = raw_df.apply(lambda x: format_conversation(x['prompt_processed'], x['response_a_processed']), axis=1)\n",
        "raw_df['conversation_b'] = raw_df.apply(lambda x: format_conversation(x['prompt_processed'], x['response_b_processed']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f80bb8f",
      "metadata": {
        "id": "6f80bb8f"
      },
      "outputs": [],
      "source": [
        "word_split = raw_df[\"conversation_a\"].apply(lambda x: x.split(' '))\n",
        "word_split.apply(lambda x: len(x)).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.90])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_split = raw_df[\"conversation_b\"].apply(lambda x: x.split(' '))\n",
        "word_split.apply(lambda x: len(x)).describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.90])"
      ],
      "metadata": {
        "id": "To4kNLrf51xr"
      },
      "id": "To4kNLrf51xr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "73c2697e",
      "metadata": {
        "id": "73c2697e"
      },
      "source": [
        "The conversations mostly have < 1000 words in each conversation. Assuming $ \\text{Tokens per conversation} = 1.5 \\times \\text{Words per conversation} $, we would need a model which can handle ~1500 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mo-_Gt67-exC",
      "metadata": {
        "id": "mo-_Gt67-exC"
      },
      "outputs": [],
      "source": [
        "def create_target_col(encoding):\n",
        "    \"\"\"\n",
        "    Create column for target labels\n",
        "    \"\"\"\n",
        "\n",
        "    if encoding == [0, 0, 1]:\n",
        "        return 'tie'\n",
        "    elif encoding == [0, 1, 0]:\n",
        "        return 'model_b'\n",
        "    elif encoding == [1, 0, 0]:\n",
        "        return 'model_a'\n",
        "\n",
        "    return np.nan\n",
        "\n",
        "raw_df['target'] = raw_df[['winner_model_a', 'winner_model_b', 'winner_tie']].apply(lambda x: create_target_col(list(x)), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up modelling architecture"
      ],
      "metadata": {
        "id": "YzjnNhim6GFd"
      },
      "id": "YzjnNhim6GFd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iU5dI8RGAo9V",
      "metadata": {
        "id": "iU5dI8RGAo9V"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Get model for embeddings\n",
        "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "load_in_4bit=USE_4BIT,\n",
        "load_in_8bit=not USE_4BIT,\n",
        "bnb_4bit_quant_type=\"nf4\" if USE_4BIT else None,\n",
        "bnb_4bit_use_double_quant=True if USE_4BIT else None,\n",
        "bnb_4bit_compute_dtype=torch.bfloat16 if USE_4BIT and torch.cuda.is_available() else None,\n",
        ")\n",
        "\n",
        "backbone = AutoModel.from_pretrained(MODEL_NAME, quantization_config=quant_config, device_map=\"auto\")\n",
        "\n",
        "# Prepare for k-bit training (fixes layer norms, casts, etc.)\n",
        "backbone = prepare_model_for_kbit_training(backbone)\n",
        "\n",
        "# Apply LoRA\n",
        "lora_cfg = LoraConfig(\n",
        "r=LORA_R,\n",
        "lora_alpha=LORA_ALPHA,\n",
        "lora_dropout=LORA_DROPOUT,\n",
        "target_modules=LORA_TARGETS,\n",
        "bias=\"none\",\n",
        "task_type=\"SEQ_CLS\", # generic; works for encoder models\n",
        ")\n",
        "backbone = get_peft_model(backbone, lora_cfg)\n",
        "backbone.print_trainable_parameters()\n",
        "\n",
        "\n",
        "hidden_size = backbone.config.hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert max_sequence_length <= model.config.max_position_embeddings, f\"Config 'max_sequence_length' must be <= the max sequence length allowed by the model i.e. {model.config.max_position_embeddings}\""
      ],
      "metadata": {
        "id": "MQ40LbePugPj"
      },
      "id": "MQ40LbePugPj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Model Architecture"
      ],
      "metadata": {
        "id": "KVhepZmjzjR6"
      },
      "id": "KVhepZmjzjR6"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
        "    summed = (last_hidden_state * mask).sum(dim=1)\n",
        "    count = mask.sum(dim=1).clamp(min=1e-9)\n",
        "    return summed / count\n",
        "\n",
        "class PairwiseBiEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Two independent encodes + comparison head -> 3-way logits.\n",
        "    Expects tokenized dicts for A and B: {input_ids, attention_mask}.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder: nn.Module, hidden_size: int, num_labels: int = 3, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4 * hidden_size, 2*hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(2 * hidden_size, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_size // 2, num_labels)\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _encode(self, input_ids, attention_mask):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Prefer mean pooling for stability across backbones\n",
        "        if hasattr(out, \"last_hidden_state\"):\n",
        "            pooled = mean_pool(out.last_hidden_state, attention_mask)\n",
        "        else:\n",
        "            # Some models expose .pooler_output\n",
        "            pooled = out.pooler_output\n",
        "        return pooled\n",
        "\n",
        "    def forward(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        # batch keys expected: a_input_ids, a_attention_mask, b_input_ids, b_attention_mask\n",
        "        hA = self._encode(batch[\"a_input_ids\"], batch[\"a_attention_mask\"])  # [B, H]\n",
        "        hB = self._encode(batch[\"b_input_ids\"], batch[\"b_attention_mask\"])  # [B, H]\n",
        "        comb = torch.cat([hA, hB, torch.abs(hA - hB), hA * hB], dim=-1)\n",
        "        logits = self.classifier(self.dropout(comb))\n",
        "        return logits"
      ],
      "metadata": {
        "id": "V9ke4STbziyW"
      },
      "id": "V9ke4STbziyW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "# Prefer mean pooling for stability across backbones\n",
        "if hasattr(out, \"last_hidden_state\"):\n",
        "    pooled = mean_pool(out.last_hidden_state, attention_mask)"
      ],
      "metadata": {
        "id": "7uq_KTQ42jRc"
      },
      "id": "7uq_KTQ42jRc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_a"
      ],
      "metadata": {
        "id": "sc01_4GP7AsT"
      },
      "id": "sc01_4GP7AsT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292269de",
      "metadata": {
        "id": "292269de"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67e14f8f",
      "metadata": {
        "id": "67e14f8f"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", quantization_config=quantization_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19565d90",
      "metadata": {
        "id": "19565d90"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IwRYQETyYDZJ",
      "metadata": {
        "id": "IwRYQETyYDZJ"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"lora_only\",\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    use_rslora = True,\n",
        "    init_lora_weights = 'eva',\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WRQfRWG2arvG",
      "metadata": {
        "id": "WRQfRWG2arvG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NPhh67GpWAZg",
      "metadata": {
        "id": "NPhh67GpWAZg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qtkHoMLnWA34",
      "metadata": {
        "id": "qtkHoMLnWA34"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cuda_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}